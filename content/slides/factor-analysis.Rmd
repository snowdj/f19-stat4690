---
title: "Factor Analysis"
draft: false
source: true
output: binb::metropolis
fontsize: 12pt
author: Max Turgeon
institute: STAT 4690--Applied Multivariate Analysis
header-includes:
  - \usefonttheme{professionalfonts}
  - \usepackage{graphicx}
  - \usepackage{tikzpagenodes}
  - \usetikzlibrary{calc}
  - \usepackage{caption}
---

```{r,setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE,
                      message = FALSE)
```

## Latent variable models

  - With PCA, we saw how we could reduce the dimension of data using the eigenvectors of the sample covariance matrix.
  - Conversely, we could construe PCA has a generative model, where the principal components give rise to the observed data.
  - **Latent Variable Models** formalise this idea:
    + Latent (i.e. unobserved) variables $\mathbf{F}$ give rise to observed data $\mathbf{Y}$ through a *specified* model.
    
## Factor Analysis {.allowframebreaks}

  - **Factor Analysis** is a special kind of latent variable model. 
  - Let $\mathbf{Y}$ be a $p$-dimensional vector with mean $\mu$ and covariance matrix $\Sigma$.
  - Let $\mathbf{F}$ be a $m$-dimensional *latent* vector.
  - The *orthogonal factor model* is given by
  $$\mathbf{Y} - \mu= L \mathbf{F} + \mathbf{E},$$
  where $L$ is a $p\times m$ *matrix of factor loadings*, and $\mathbf{E}$ is a $p$-dimensional vector of *errors*.
  - $\mathbf{F}$ are also called *common factors*; $\mathbf{E}$ are also called *specific factors*.
  - **Note**: This is essentially a multivariate regression model, but where the covariates are unobserved.
  
## Assumptions {.allowframebreaks}

  - The model above is generally not identifiable, since there are too many parameters.
  - We therefore need to impose further restrictions:
    + $E(\mathbf{F}) = 0$
    + $\mathrm{Cov}(\mathbf{F}) = I$
    + $E(\mathbf{E}) = 0$
    + $\mathrm{Cov}(\mathbf{E}) = \Psi =\begin{pmatrix} \psi_1 & 0 & \cdots & 0 \\ 0 & \psi_2 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots\\0 & 0 & \cdots & \psi_p \end{pmatrix}$
    + $\mathrm{Cov}(\mathbf{F}, \mathbf{E}) = 0$
- In other words:
  + Both common and specific factors have mean zero;
  + They are uncorrelated;
  + The common factors are mutually uncorrelated and standardised;
  + The specific factors each affect only one observed variable.

## Structured Covariance {.allowframebreaks}

  - As a consequence of these assumptions, we can derive an assumption on the structure of $\Sigma = \mathrm{Cov}(\mathbf{Y})$:
  \begin{align*}
  \Sigma &= E\left( (\mathbf{Y} - \mu)(\mathbf{Y} - \mu)^T \right)\\
  &= E\left( (L \mathbf{F} + \mathbf{E})(L \mathbf{F} + \mathbf{E})^T \right)\\
  &= LE(\mathbf{F}\mathbf{F}^T)L + E(\mathbf{E}\mathbf{F}^T)L^T + LE(\mathbf{F}\mathbf{E}^T) + E(\mathbf{E}\mathbf{E}^T)\\
  &= LIL^T + 0L^T + L0 + \Psi\\
  &= LL^T + \Psi.
  \end{align*}
  - Similarly, we can show that
  $$ \mathrm{Cov}(\mathbf{Y}, \mathbf{F}) = L.$$
  - If we write $\ell_{ij}$ for the $(i,j)$-th element of $L$, we see that 
  $$ \mathrm{Var}(Y_i) = \sum_{k=1}^m \ell_{ik}^2 + \psi_i.$$
  - Crucially, these assumptions are **testable**. In other words, we can check whether they are reasonable for our data.
  
## Example {.allowframebreaks}

  - Let's look at an example where there is no solution. 
  - Assume $p=3$, $m=1$, with 
  $$\Sigma = \begin{pmatrix} 1 & 0.9 & 0.7\\0.9 & 1 & 0.4\\0.7 & 0.4 & 1\end{pmatrix}.$$
  - From our assumptions on the covariance structure, we derive a system of equations
  $$\begin{matrix}
  1 = \ell_{11}^2 + \psi_1 & 0.9 = \ell_{11}\ell_{21} & 0.7 = \ell_{11}\ell_{31} \\
  & 1 = \ell_{22}^2 + \psi_2 & 0.4= \ell_{21}\ell_{31}\\
  & & 1 = \ell_{33}^2 + \psi_3
  \end{matrix}$$
  - From $0.7 = \ell_{11}\ell_{31}$ and $0.4= \ell_{21}\ell_{31}$, we get 
  $$ \ell_{21} = \frac{0.4}{0.7}\ell_{11}.$$
  - But since $0.9 = \ell_{11}\ell_{21}$, we can conclude
  that
  $$ \ell_{11} = \pm 1.255.$$
  - However, since the first component $Y_1$ has unit variance, $\ell_{11} = \mathrm{Corr}(Y_1, F_1)$, and therefore the correlation is out of bounds.
  - Similarly, we get
  $$ \psi_1 = 1 - \ell_{11}^2 = 1 - 1.575 = -0.575.$$
  - But since $\psi_1$ is the variance of the first error term, we once again get a non-sensical solution.

## Factor Rotation {.allowframebreaks}

  - Even with our assumptions above, our model is still not uniquely identified.
  - Let $T$ be an $m\times m$ orthogonal matrix. We have
  \begin{align*}
  \mathbf{Y} - \mu &= L \mathbf{F} + \mathbf{E}\\
  &= L TT^T\mathbf{F} + \mathbf{E}\\
  &= \tilde{L}\tilde{\mathbf{F}} + \mathbf{E},\\
  \end{align*}
  where $\tilde{L}=LT$ and $\tilde{\mathbf{F}}=T^T\mathbf{F}$.
  - Both models lead to the same covariance matrix:
  $$ \Sigma = LL^T + \Psi = LTT^TL^T + \Psi = \tilde{L}\tilde{L}^T + \Psi.$$
  - As we will see, this will turn out to be a blessing in disguise:
    + We will impose a uniqueness condition to get one solution.
    + Then we will rotate our solution using $T$ to improve interpretation.
  
## Estimation--Principal Component Method {.allowframebreaks}

  - Recall the spectral decomposition of the covariance matrix:
  $$ \Sigma = \sum_{i=1}^p \lambda_i w_i w_i^T,$$
  with $\lambda_1\geq\cdots\geq\lambda_p$.
  - If we let $W$ be the matrix whose $i$-th column is $\sqrt{\lambda_i} w_i$, we can rewrite the spectral decomposition as
  $$ \Sigma = W W^T.$$
  - In other words, if we let $m=p$ and $\Psi = 0$, we see that we recover the orthogonal factor model with $L=W$.
  - Of course, this is not very satisfactory, as the dimension of the common factors is the same as that of the original data.
  - Instead, we select $m < p$ using one of the methods we discussed with PCA and we approximate
  $$\Sigma \approx \sum_{i=1}^m \lambda_i w_i w_i^T.$$
  - If we let $L$ be the $p\times m$ matrix whose $i$-th column is $\sqrt{\lambda_i} w_i$, we can estimate $\Psi$ as follows:
  $$\psi_i = \sigma_{ii} - \sum_{j=1}^m\ell_{ij}^2.$$

### Algorithm

  1. Let $\hat{\lambda}_1\>\cdots>\hat{\lambda}_p$ and $\hat{w}_1,\ldots, \hat{w}_p$ be the eigenvalues and eigenvectors of the covariance matrix $S_n$.
  2. Select $m$ using one of the PCA criteria.
  3. Estimate $\hat{L}$ with the $p\times m$ matrix whose $i$-th column is $\sqrt{\hat{\lambda}_i} \hat{w}_i$.
  4. Estimate $\hat{\Psi}$ with the diagonal elements of $S_n - \hat{L}\hat{L}^T$.
  
## Example {.allowframebreaks}

  - We are going to use the `bfi` dataset in the `psych` package.
  - Contains data on 25 personality items grouped in 5 categories:
    + A (Agreeableness)
    + C (Conscientiousness)
    + E (Extraversion) 
    + N (Neuroticism)
    + O (Openness)
  
```{r}
library(psych)

dim(bfi)
tail(names(bfi), n = 3)
```

```{r message = FALSE}
library(tidyverse)

# Remove demographic variable and keep complete data
data <- bfi %>% 
  select(-gender, -education, -age) %>% 
  filter(complete.cases(.))
```

```{r}
cor.plot(cor(data))
```

```{r}
decomp <- prcomp(data)
summary(decomp)$importance[,1:3]

cum_prop <- decomp %>% 
  summary %>% 
  .[["importance"]] %>% 
  .["Cumulative Proportion",]

which(cum_prop > 0.8)[1]
```

```{r}
Lhat <- decomp$rotation[,1:14] %*% 
  diag(decomp$sdev[1:14])
Psi_hat <- diag(cov(data) - tcrossprod(Lhat))
```


```{r}
# Sum squared error
sum((cov(data) - tcrossprod(Lhat) - diag(Psi_hat))^2)
# Compare to the total variance
sum(diag(cov(data)))

# Our FA model explains:
sum(colSums(Lhat^2)/sum(diag(cov(data))))
```

## Comments

  - In our example above, we saw that 14 factors explained 82% of the total variance.
  - Two common default values for $m$ in statistical softwares:
    + Number of positive eigenvalues of the sample covariance matrix.
    + Number of eigenvalues greater than one for the sample correlation matrix.
  - In our example, the first criterion would lead to $m=p$, which is not very helpful
  
## Example (cont'd) {.allowframebreaks}

```{r}
(m <- sum(eigen(cor(data))$values > 1))

Lhat <- decomp$rotation[,seq_len(m)] %*% 
  diag(decomp$sdev[seq_len(m)])
Psi_hat <- diag(cov(data) - tcrossprod(Lhat))

# Sum squared error
sum((cov(data) - tcrossprod(Lhat) - diag(Psi_hat))^2)
# Compare to the total variance
sum(diag(cov(data)))
```


```{r}
# Our FA model explains:
sum(colSums(Lhat^2)/sum(diag(cov(data))))
```

```{r}
# We can also  visualize the fit
Sn <- cov(data)
Sn_fit <- tcrossprod(Lhat) + diag(Psi_hat)

library(lattice)
levelplot(Sn - Sn_fit)
```

```{r}
# Or if you prefer % difference
levelplot((Sn - Sn_fit)/Sn)
```

## Estimation--Maximum Likelihood Method {.allowframebreaks}

  - This estimation method assumes that both common factors $\mathbf{F}$ and specific factors $\mathbf{E}$ follow a multivariate normal distribution
    + $\mathbf{F}\sim N_m(0, I)$
    + $\mathbf{E}\sim N_p(0, \Psi)$
  - From this assumption, it follows that $\mathbf{Y}$ also follows a multivariate normal distribution
    + $\mathbf{Y}\sim N_p(\mu, LL^T + \Psi)$
  - Therefore, we can write down the likelihood in terms of both $L$ and $\Psi$.
  - However, because of the factor rotation problem, we need to impose a constraint in order to obtain a unique solution:
    + $L^T\Psi^{-1}L = \Delta$ is diagonal.
  - Given this assumption, the maximum likelihood estimates $\hat{L}$ and $\hat{\Psi}$ can be found using an iterative algorithm.
  - We will not go into the details of the algorithm (but see Johnson & Wichern, Supplement 9A if interested).
    + Instead, we will rely on the `R` function `stats::factanal`.
    
## Example (cont'd) {.allowframebreaks}

```{r}
# CAREFUL: uses correlation matrix
fa_decomp <- factanal(data, factors = m,
                      rotation = 'none')

# Uniquenesses are the diagonal elements 
# of the matrix Psi
Psi_mle <- fa_decomp$uniquenesses
Lmle <- fa_decomp$loadings
```


```{r}
# We get an estimate of the correlation
R_mle <- tcrossprod(Lmle) + diag(Psi_mle)

sum((cor(data) - R_mle)^2)

# Our FA model explains:
sum(colSums(Lmle^2)/ncol(data))
```

```{r}
levelplot(cor(data) - R_mle)
```

```{r}
# To factor the covariance matrix
# Use psych::fa
fa_decomp <- psych::fa(data, nfactors = m,
                       rotate = "none",
                       covar = TRUE,
                       fm = "ml")

# Extract estimates
Psi_mle <- fa_decomp$uniquenesses
Lmle <- fa_decomp$loadings
```


```{r}
# We get an estimate of the covariance
Sn_mle <- tcrossprod(Lmle) + diag(Psi_mle)

sum((Sn - Sn_mle)^2)

# Our FA model explains:
sum(colSums(Lmle^2)/sum(diag(Sn)))
```

```{r}
levelplot(Sn - Sn_mle)
```

```{r}
# Compare MLE with PC estimate
levelplot(Sn_fit - Sn_mle)
```

## Comments about estimation {.allowframebreaks}

  - There are other methods of estimating the loadings and the "uniquenesses"
    + Ordinary Least Squares
    + Weighted OLS
    + Principal factor
  - With so many choices of estimation methods, it can be hard to compare statistical softwares
    + And you also have to read the manual in order to know what is going on...
  - You also always have the choice between factoring the covariance or the correlation matrix
    + Which one you choose depends on the commensurability of your variables (just like in PCA)
  - Finally, it's always a good idea to compare the output of multiple estimation strategies
    + If your model is a good fit, you should get a similar answer regardless of the method.
    
## Factor Rotation Redux {.allowframebreaks}

  - As we saw earlier, any orthogonal matrix $T$ gives rise to the same factor analysis model
  $$ \Sigma = LL^T + \Psi = LTT^TL^T + \Psi = \tilde{L}\tilde{L}^T + \Psi.$$
  - In other words, we cannot choose $T$ to maximise the goodness of fit.
    + We need another criterion
  - Intuitively, to ease interpretation, we want each variable to have large loadings for one factor and negligible loadings for the other ones.
    + \url{https://maxturgeon.ca/f19-stat4690/factor_rotation.gif}
  - One common analytic criterion that formalises this idea is the **varimax criterion**.
    + Resulting loadings are called *varimax loadings*
  - We have
  $$ \mathrm{VARIMAX} \propto \sum_{j=1}^m \binom{\mbox{Variance of squares of scales loadings}}{\mbox{for }j\mbox{-th factor}}$$
  - More precisely:
    + Let $\tilde{\ell}_{ij}$ be the $(i,j)$-th entry of the matrix $\tilde{L}=LT$. In other words, $\tilde{\ell}_{ij}$ depends on $T$.
    + Let $\tilde{h}_i^2 = \sum_{j=1}^m \tilde{\ell}_{ij}^2$.
    + Define the scaled loadings $\tilde{\ell}^*_{ij}=\tilde{\ell}_{ij}/h_i$.
    + The varimax criterion $V$ is given as
    $$V = \frac{1}{p}\sum_{j=1}^m\left(\sum_{i=1}^p\tilde{\ell}^{*4}_{ij} - \frac{1}{p}\left(\sum_{i=1}^p\tilde{\ell}^{*2}_{ij}\right)^2\right).$$
  - In `R`, you can compute the rotated loadings using the `stats::varimax` function. Alternatively, the function `stats::factanal` can compute the rotation for you as part of the factor analysis (and so does `psych::fa`).
  
## Example (cont'd) {.allowframebreaks}

```{r}
# Let's start with m=2 for visualization
fa_decomp <- factanal(data, factors = 2,
                      rotation = 'none')

initial_loadings <- fa_decomp$loadings
varimax_loadings <- varimax(initial_loadings)
```

```{r echo = FALSE}
par(mfrow = c(1, 2))

plot(initial_loadings, type = 'n',
     xlim = c(-0.5, 0.8),
     ylim = c(-0.6, 0.6),
     main = "Initial")
text(initial_loadings, label = rownames(initial_loadings))
abline(h = 0, v = 0)

plot(varimax_loadings$loadings, type = 'n',
     xlim = c(-0.5, 0.8),
     ylim = c(-0.6, 0.6),
     main = "Varimax")
text(varimax_loadings$loadings, 
     label = rownames(varimax_loadings$loadings))
abline(h = 0, v = 0)

par(mfrow = c(1,1))
```

```{r}
# You can extract the matrix T
varimax_loadings$rotmat

# We can also get the angle of rotation
acos(varimax_loadings$rotmat[1,1])
```

```{r}
# In more dimensions
fa_decomp <- factanal(data, factors = m,
                      rotation = 'varimax')

levelplot(unclass(fa_decomp$loadings),
          xlab = "", ylab = "")
```

## Comments

  - As with estimation, there are many more rotation methods.
    + See for example the help page `?GPArotation::rotations`
  - One particular class of rotations are called *oblique*
    + The matrix $T$ is no longer constrained to be orthogonal.
  - Factor rotation is especially useful with loadings obtained through MLE
    + Recall the constraint on $L^T\Psi^{-1}L$ being diagonal
  - Factor rotations are also sometimes used with PCA loadings.
  
## Selecting the number of factors {.allowframebreaks}

  - We have discussed some of these strategies already.
  - We could select the minimum number of factors $m$ that explains a certain proportion of variance.
    + Let $L$ be the matrix of loadings and for each factor $j$ we let 
    $$\frac{\sum_{i=1}^p \ell_{ij}^2}{\mathrm{tr}(S_n)}$$ 
    be the proportion of total sample variance due to the $j$-th factor.
    + In other words, when computing the proportion of variance due to the factor model, we do not include the variance coming from the error term.
  - We could use a scree plot approach:
    + Fit different factor analysis models with a varying value of $m$ and plot the total proportion of variance explained as a function of $m$.
    + You could also select $m$ using a scree plot based on the eigenvalues of $S_n$ (just as in PCA).
  - We could also select $m$ as the number of eigenvalues of $S_n$ that are larger than the average eigenvalue.
  - We could also select only factors that can be explained by the researcher.
    + This requires a lot of domain-knowledge expertise.
  
## Example (cont'd) {.allowframebreaks}

```{r}
# First, let's look at the average eigenvalue
decomp <- eigen(Sn, symmetric = TRUE,
                only.values = TRUE)

mean(decomp$values)

sum(decomp$values > mean(decomp$values))
```


```{r}
# We will go from 1 factor to 15
prop_var_explained <- purrr::map_df(
  seq(1, 15),  function(m) {
    fa_decomp <- factanal(data, factors = m)
    Lmle <- fa_decomp$loadings
    prop <- sum(colSums(Lmle^2))/ncol(data)
    
    data.frame(
      prop = prop,
      m = m
    )})
```


```{r}
prop_var_explained %>% 
  ggplot(aes(m, prop)) + 
  geom_point() + 
  geom_line() + 
  theme_minimal() +
  expand_limits(y = 0) +
  geom_vline(xintercept = 6,
             linetype = 'dotted')
```

## Information criteria

  - We will discuss two more approaches.
  - First, if we use the MLE estimation method, we can also use information criteria.
    + Bayesian Information Criterion (BIC)
    + empirically-derived BIC (eBIC)
    + sample-size adjusted BIC (saBIC)
  - These 3 criteria have been implemented in `psych::fa`.
  - To use these criteria, fit the model with different values of $m$ and select the model with the smallest value of the criterion.

## Example (cont'd) {.allowframebreaks}

```{r warning=FALSE}
inform_crit <- purrr::map_df(
  seq(1, 15),  function(m) {
    fa_decomp <- psych::fa(data, nfactors = m,
                           fm = 'ml')
    data.frame(
      BIC = fa_decomp$BIC, eBIC = fa_decomp$EBIC,
      SABIC = fa_decomp$SABIC, m = m
    )})
```

```{r}
inform_crit %>% 
  gather(Criteria, value, -m) %>% 
  ggplot(aes(m, value, colour = Criteria)) + 
  geom_point() + 
  geom_line() + 
  theme_minimal() +
  theme(legend.position = 'top') +
  geom_vline(xintercept = 6,
             linetype = 'dotted')
```

```{r}
inform_crit %>% 
  gather(Criteria, value, -m) %>% 
  group_by(Criteria) %>% 
  filter(value == min(value)) %>% 
  select(-value) %>% 
  spread(Criteria, m)
```

## Large sample test {.allowframebreaks}

  - The second alternative approach is based on hypothesis testing.
  - We will perform a different test for each value of $m$.
  - For a fixed $m$, the *null hypothesis* is that the corresponding factor model
  $$ \Sigma = LL^T + \Psi$$
  is correct.
  - The alternative hypothesis is that $\Sigma$ is *unstructured*.
    + Note that the different factor models with different $m$ are not nested.
  - Under $H_0$, the likelihood of the model is proportional to
  $$\lvert \hat{L}\hat{L}^T + \hat{\Psi}\rvert^{-n/2}\exp\left(-\frac{1}{2}n\mathrm{tr}\left((\hat{L}\hat{L}^T + \hat{\Psi})^{-1}S_n\right)\right).$$
  - Therefore, the likelihood ratio statistic is
  $$-2\log\Lambda = n\log\left(\frac{\lvert \hat{L}\hat{L}^T + \hat{\Psi}\rvert}{\lvert S_n\rvert}\right).$$
  - Under $H_0$, this follows a $\chi^2$ distribution with $\nu$ degrees of freedom, where
  $$\nu = \frac{1}{2}\left((p-m)^2 - p - m\right).$$

## Example (cont'd) {.allowframebreaks}

```{r}
m <- 6
fa_decomp <- psych::fa(data, nfactors = m,
                       covar = TRUE,
                       fm = 'ml')
```


```{r}
Sn <- cov(data)
Sn_mle <- tcrossprod(fa_decomp$loadings) + 
  diag(fa_decomp$uniquenesses)

test_stat <- nrow(data) * (
  log(det(Sn_mle)) -
    log(det(Sn))
)
```


```{r}
p <- ncol(data)
nu <- 0.5*((p-m)^2 - p - m)

test_stat > qchisq(0.95, df = nu)
```

## Comments

  - The hypothesis test approach is nor really recommended:
    + It tends to select too many factors, especially when the sample size is large.
  - If a factor model is good model for a given dataset, the "average eigenvalue" method should given a similar answer to the scree plot approach.
  
## Factor scores {.allowframebreaks}

  - Recall the original model:
  $$\mathbf{Y} - \mu= L \mathbf{F} + \mathbf{E}.$$
  - We have discussed ways to estimate $L$ and the covariance matrix of $\mathbf{E}$, but we have not covered how to estimate $\mathbf{F}$ yet.
  - In a typical analysis, we may be mainly interested in estimating and inspecting the loadings.
  - But it may be useful to estimate the common factors for diagnostic purposes (e.g. model fit) or as input into a second analysis (e.g. two-stage modeling).
  - We will discuss two estimation strategies:
    1. Weighted Least Squares
    2. Regression
    
## Weighted Least Squares Method {.allowframebreaks}

  - First, assume that we know $L,\mu,\Psi$.
  - If we regard $\mathbf{E}$ as error terms, we would want to minimise
  $$(\mathbf{Y} - \mu - L \mathbf{F})^T(\mathbf{Y} - \mu - L \mathbf{F}).$$
  - However, since the errors terms have unequal variance (i.e. heteroscedasticity), we may have a better performance by weighting the $j$-th term by the inverse of $\psi_j$. We therefore have a different criterion to minimise:
  $$(\mathbf{Y} - \mu - L \mathbf{F})^T\Psi^{-1}(\mathbf{Y} - \mu - L \mathbf{F}).$$
  - Minimising with respect to $\mathbf{F}$, we get
  $$\hat{\mathbf{F}} = (L^T\Psi^{-1}L)^{-1}L^T\Psi^{-1}(\mathbf{Y} - \mu).$$
  - Using this computation as a heuristic, we can replace $L,\Psi,\mu$ by $\hat{L},\hat{\Psi}, \bar{\mathbf{Y}}$, and for each observation $\mathbf{Y}_i$, we get an estimate
  $$\hat{\mathbf{F}}_j = (\hat{L}^T\hat{\Psi}^{-1}\hat{L})^{-1}\hat{L}^T\hat{\Psi}^{-1}(\mathbf{Y}_j - \bar{\mathbf{Y}}).$$
  - **Note**: When $\hat{L},\hat{\Psi}$ are obtained using the principal component approach, it is customary to use the *unweighted* approach, in which case the estimates $\hat{\mathbf{F}}_j$ are simply the principal components of $\mathbb{Y}$.
  
## Example (cont'd) {.allowframebreaks}

```{r, message = FALSE}
m <- 6
fa_decomp <- psych::fa(data, nfactors = m,
                       covar = TRUE,
                       fm = 'ml')
```


```{r, message = FALSE}
# Extract estimates
InvPsi <- diag(fa_decomp$uniquenesses^-1)
Lhat <- fa_decomp$loadings

hat_mat <- solve(t(Lhat) %*% InvPsi %*% Lhat) %*%
  t(Lhat) %*% InvPsi

scores <- scale(data, center = TRUE,
                scale = FALSE) %*% t(hat_mat)
```


```{r message = FALSE}
GGally::ggpairs(as.data.frame(scores))
```

## Regression Method {.allowframebreaks}

  - As before, assume that we know $L,\mu,\Psi$.
  - Under the normality assumptions $\mathbf{F}\sim N_m(0, I)$ and $\mathbf{E}\sim N_p(0, \Psi)$, we can conclude that $(\mathbf{F}, \mathbf{E})$ and therefore $(\mathbf{Y} - \mu, \mathbf{F})$ are also jointly normal.
  - More precisely, $(\mathbf{Y} - \mu, \mathbf{F})$ is $N_{m+p}(0, \Sigma^*)$, where
  $$\Sigma^* = \begin{pmatrix} LL^T + \Psi & L\\L^T & I\end{pmatrix}.$$
  - Using our result on conditional distributions (see slides on Multivariate Normal Distribution), we know that the distribution of $\mathbf{F}$ given $\mathbf{Y}$ has:
    + Mean $L^T(LL^T + \Psi)^{-1}(\mathbf{Y} - \mu)$;
    + Covariance $I - L^T(LL^T + \Psi)^{-1}L$.
  - Therefore, given estimates $\hat{L},\hat{\Psi}$, we get an estimate of the scores as:
  $$\hat{\mathbf{F}}_j = \hat{L}^T\left(\hat{L}\hat{L}^T + \hat{\Psi}\right)^{-1}(\mathbf{Y}_j - \bar{\mathbf{Y}}).$$
  - To mitigate the effects of model misspecification, it is common to replace $\hat{L}\hat{L}^T + \hat{\Psi}$ by the sample covariance matrix $S_n$.
  
## Example (cont'd) {.allowframebreaks}

```{r, message = FALSE}
scores_reg <- scale(data, center = TRUE,
                    scale = FALSE) %*% 
  solve(Sn) %*% Lhat
```


```{r message = FALSE}
GGally::ggpairs(as.data.frame(scores_reg))
```

```{r}
# Let's look at agreement
round(cor(scores, scores_reg), 2)
```


```{r}
# Or graphically
plot(as.vector(scores),
     as.vector(scores_reg),
     xlab = "WLS",
     ylab = "Regression")
abline(a = 0, b = 1, 
       col='red',
       lty = 2)
```

```{r, echo = FALSE}
inner_join(
  as.data.frame(scores_reg) %>% 
    mutate(ID = row_number()) %>% 
    gather(Factor, Score, -ID) %>% 
    rename(Regression = Score),
  as.data.frame(scores) %>% 
    mutate(ID = row_number()) %>% 
    gather(Factor, Score, -ID) %>% 
    rename(WLS = Score),
  by = c("ID", "Factor")
) %>% 
  ggplot(aes(WLS, Regression, colour = Factor)) + 
  geom_abline(slope = 1, intercept = 0,
              linetype = 'dashed') +
  geom_point(alpha = 0.5) +
  geom_smooth(method = 'lm', se = FALSE) +
  theme_minimal() +
  theme(legend.position = 'top')
```

## Comments

  - Neither method is uniformly superior.
    + On the other hand, the regression method uses normal theory to derive its heuristic.
  - As for other topics we discussed earlier, there are other methods for estimating the scores
    + Harman's method
    + Anderson's method
    + ten Berge's method
  - What is important to remember is that there is no best way to estimate the scores, and different methods optimise different criteria
    + E.g. Anderson's method looks for *uncorrelated* scores.
    
## Final comments on Factor Analysis {.allowframebreaks}

  - The type of Factor Analysis we discussed is called **Exploratory**.
    + **Confirmatory** FA would make strong assumptions about the nature of the latent factors and perform statistical inference.
  - At every stage of FA, there are choices to make: estimation method, number of factors, factor rotation, score estimation.
    + This makes FA more of an art than a science.
  - But there are still some general principles you can follow.
  
### General Strategy for FA

\begin{enumerate}
  \item Perform a \emph{Principal Component} Factor Analysis.
  \begin{itemize}
    \item It is simple to run.
    \item It will help you find potential outliers.
  \end{itemize}
  \item Perform a \emph{Maximum Likelihood} Factor Analysis.
  \begin{itemize}
    \item Try a varimax rotation to see if it makes sense.
  \end{itemize}
  \item Compare the solution of both methods to see if they generally agree.
  \item Repeat for different values of $m$ and check if adding more factors improve interpretation of the results.
  \item For large datasets, you can split your data, run the same model on both subsets, and compare the loadings to see if they generally agree.
\end{enumerate}

## A different example {.allowframebreaks}

  - The dataset: `state.x77`. It contains general information about all 50 states.
    + Population
    + Income per capita
    + Illiteracy rate
    + Life Expectancy
    + Murder rate
    + High-School Graduation rate
    + Average number of days below 0C
    + Total area
    
```{r}
state.x77[1:5,1:4]
```

```{r}
library(GGally)
data <- as.data.frame(state.x77)
ggpairs(data)
```

```{r}
# Potential outliers?
data %>% 
  rownames_to_column('State') %>%
  top_n(Population, n = 2)

data %>% 
  rownames_to_column('State') %>%
  top_n(Area, n = 2)
```

```{r}
# 1. Principal Component Factor analysis
decomp <- princomp(data, cor = TRUE)
decomp
```

```{r}
biplot(decomp)
```


```{r}
m <- 3
Lhat <- decomp$loadings[,seq_len(m)] %*% 
  diag(decomp$sdev[seq_len(m)])
colnames(Lhat) <- paste0("PC", 1:3)
Psi_hat <- diag(cov(data) - tcrossprod(Lhat))

# Our FA model explains:
sum(colSums(Lhat^2))/ncol(data)
```

```{r}
# We can also  visualize the fit
Rn <- cor(data)
Rn_fit <- tcrossprod(Lhat) + diag(Psi_hat)

levelplot(Rn - Rn_fit)
```

```{r}
scores_pc <- scale(data, center = TRUE,
                    scale = TRUE) %*% 
  solve(Rn) %*% Lhat
```

```{r}
ggpairs(as.data.frame(scores_pc))
```

```{r}
# What state has the outlying values?
scores_pc %>% 
  as.data.frame %>% 
  rownames_to_column('State') %>%
  filter(PC2 > 4 | PC3 < -4)
```

```{r}
# 2. Maximum Likelihood Factor analysis
fa_decomp <- factanal(data, factors = m)
```


```{r}
# Extract estimates
Psi_mle <- fa_decomp$uniquenesses
Lmle <- fa_decomp$loadings

# Our FA model explains:
sum(colSums(Lmle^2))/ncol(data)
```

```{r}
# We can also  visualize the fit
Rn_mle <- tcrossprod(Lmle) + diag(Psi_mle)

levelplot(Rn - Rn_mle)
```

```{r}
scores_mle <- scale(data, center = TRUE,
                    scale = TRUE) %*% 
  solve(Rn) %*% Lmle
```

```{r}
ggpairs(as.data.frame(scores_mle))
```

```{r}
# 3. Compare both loadings
round(cor(scores_pc, scores_mle), 2)
```

```{r}
levelplot(Lhat,
          xlab = "", ylab = "")
```

```{r}
# Let's rotate the PC loadings
Lhat <- varimax(Lhat)$loadings
scores_pc <- scale(data, center = TRUE,
                    scale = TRUE) %*% 
  solve(Rn) %*% Lhat
```

```{r}
# Compare both loadings again
round(cor(scores_pc, scores_mle), 2)
```

```{r}
levelplot(unclass(Lhat),
          xlab = "", ylab = "")
```

```{r}
levelplot(unclass(Lmle),
          xlab = "", ylab = "")
```

```{r, echo = FALSE, eval = FALSE}
inner_join(
  as.data.frame(scores_pc) %>% 
    mutate(ID = row_number()) %>% 
    gather(Factor, Score, -ID) %>% 
    rename(PC = Score) %>% 
    mutate(Factor = stringr::str_replace(Factor, "PC", "Factor")),
  as.data.frame(scores_mle) %>% 
    mutate(ID = row_number()) %>% 
    gather(Factor, Score, -ID) %>% 
    rename(MLE = Score),
  by = c("ID", "Factor")
) %>% 
  ggplot(aes(PC, MLE, colour = Factor)) + 
  geom_abline(slope = 1, intercept = 0,
              linetype = 'dashed') +
  geom_point(alpha = 0.5) +
  geom_smooth(method = 'lm', se = FALSE) +
  theme_minimal() +
  theme(legend.position = 'top')
```

```{r}
# 4. Compare different m
# Let's start with scree plot
screeplot(decomp, type = 'l')
```

```{r warning = FALSE}
# Then let's look at information criteria
inform_crit <- purrr::map_df(
  seq_len(4),  function(m) {
    fa_decomp <- psych::fa(data, nfactors = m,
                           fm = 'ml')
    data.frame(
      BIC = fa_decomp$BIC, eBIC = fa_decomp$EBIC,
      SABIC = fa_decomp$SABIC, m = m
    )})
```

```{r}
inform_crit %>% 
  gather(Criteria, value, -m) %>% 
  ggplot(aes(m, value, colour = Criteria)) + 
  geom_point() + 
  geom_line() + 
  theme_minimal() +
  theme(legend.position = 'top')
```

```{r}
inform_crit %>% 
  gather(Criteria, value, -m) %>% 
  group_by(Criteria) %>% 
  filter(value == min(value)) %>% 
  select(-value) %>% 
  spread(Criteria, m)
```

```{r}
m <- 2
Lhat <- decomp$loadings[,seq_len(m)] %*% 
  diag(decomp$sdev[seq_len(m)])
colnames(Lhat) <- paste0("PC", 1:2)
Psi_hat <- diag(cov(data) - tcrossprod(Lhat))

Lhat <- varimax(Lhat)$loadings
scores_pc <- scale(data, center = TRUE,
                    scale = TRUE) %*% 
  solve(Rn) %*% Lhat
```


```{r}
# MLE
fa_decomp <- factanal(data, factors = m)
Psi_mle <- fa_decomp$uniquenesses
Lmle <- fa_decomp$loadings

scores_mle <- scale(data, center = TRUE,
                    scale = TRUE) %*% 
  solve(Rn) %*% Lmle
```

```{r}
# Compare both loadings again
round(cor(scores_pc, scores_mle), 2)
```

```{r}
levelplot(unclass(Lhat),
          xlab = "", ylab = "")
```

```{r}
levelplot(unclass(Lmle),
          xlab = "", ylab = "")
```

```{r}
plot(scores_mle, type = 'n')
text(scores_mle, labels = rownames(scores_mle))
abline(h = 0, v = 0, lty = 2)
```


```{r}
# Let's plot the factors on the map
library(maps)
states <- map_data("state")

data_plot <- scores_mle %>%
  as.data.frame() %>% 
  rownames_to_column("region") %>% 
  mutate(region = tolower(region)) %>% 
  inner_join(states, by = "region")
```


```{r}
ggplot(data = data_plot) + 
  geom_polygon(aes(x = long, y = lat, 
                   fill = Factor1, 
                   group = group)) +
  coord_fixed(1.3) +
  ggthemes::theme_map() + 
  ggtitle("First Factor")
```

```{r}
ggplot(data = data_plot) + 
  geom_polygon(aes(x = long, y = lat, 
                   fill = Factor2, 
                   group = group)) +
  coord_fixed(1.3) +
  ggthemes::theme_map() + 
  ggtitle("Second Factor")
```