---
title: "Multivariate Distributions"
draft: true
output: binb::metropolis
fontsize: 12pt
author: Max Turgeon
institute: STAT 4690--Applied Multivariate Analysis
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```

# Review of basic concepts

## Joint distributions

  - Let $X$ and $Y$ be two random variables. 
  - The *joint distribution function* of $X$ and $Y$ is 
  $$ F(x, y) = P(X \leq x, Y \leq y).$$
  - We can recover the *marginal distributions*:
  $$ F(x) = \lim_{y\to\infty} F(x, y).$$
  - More generally, let $Y_1,\ldots,Y_p$ be $p$ random variables. Their *joint distribution function* is 
  $$ F(y_1, \ldots, y_p) = P(Y_1 \leq y_1, \ldots, Y_p \leq y_p).$$
  
## Joint densities

  - If $F$ is absolutely continuous almost everywhere, there exists a function $f$ called the *density* such that
  $$ F(y_1, \ldots, y_p) = \int_{-\infty}^{y_1} \cdots \int_{-\infty}^{y_p} f(u_1, \ldots, u_p) du_1 \ldots du_p.$$
  - The *joint moments* are defined as
  $$ E(Y_1^{n_1}\ldots Y_p^{n_p}) = \int_{-\infty}^{\infty} \cdots \int_{-\infty}^{\infty} u_1^{n_1}\ldots u_p^{n_p}f(u_1, \ldots, u_p) du_1 \ldots du_p.$$
  - **Exercise**: Show that this is consistent with the univariate definition of $E(Y_1^{n_1})$, i.e. $n_2=\cdots=n_p=0$.
  
## Statistical Independence

  - The variables $Y_1,\ldots,Y_p$ are said to be *mutually independent* if
  $$ F(y_1, \ldots, y_p) = F(y_1) \cdots F(y_p).$$
  
## Conditional Distributions
