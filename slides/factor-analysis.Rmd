---
title: "Factor Analysis"
draft: false
source: true
output: binb::metropolis
fontsize: 12pt
author: Max Turgeon
institute: STAT 4690--Applied Multivariate Analysis
header-includes:
  - \usefonttheme{professionalfonts}
  - \usepackage{graphicx}
  - \usepackage{tikzpagenodes}
  - \usetikzlibrary{calc}
  - \usepackage{caption}
---

```{r,setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```

## Latent variable models

  - With PCA, we saw how we could reduce the dimension of data using the eigenvectors of the sample covariance matrix.
  - Conversely, we could construe PCA has a generative model, where the principal components give rise to the observed data.
  - **Latent Variable Models** formalise this idea:
    + Latent (i.e. unobserved) variables $\mathbf{F}$ give rise to observed data $\mathbf{Y}$ through a *specified* model.
    
## Factor Analysis {.allowframebreaks}

  - **Factor Analysis** is a special kind of latent variable model. 
  - Let $\mathbf{Y}$ be a $p$-dimensional vector with mean $\mu$ and covariance matrix $\Sigma$.
  - Let $\mathbf{F}$ be a $m$-dimensional *latent* vector.
  - The *orthogonal factor model* is given by
  $$\mathbf{Y} - \mu= L \mathbf{F} + \mathbf{E},$$
  where $L$ is a $p\times m$ *matrix of factor loadings*, and $\mathbf{E}$ is a $p$-dimensional vector of *errors*.
  - $\mathbf{F}$ are also called *common factors*; $\mathbf{E}$ are also called *specific factors*.
  - **Note**: This is essentially a multivariate regression model, but where the covariates are unobserved.
  
## Assumptions {.allowframebreaks}

  - The model above is generally not identifiable, since there are too many parameters.
  - We therefore need to impose further restrictions:
    + $E(\mathbf{F}) = 0$
    + $\mathrm{Cov}(\mathbf{Y}) = I$
    + $E(\mathbf{E}) = 0$
    + $\mathrm{Cov}(\mathbf{Y}) = \Psi =\begin{pmatrix} \psi_1 & 0 & \cdots & 0 \\ 0 & \psi_2 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots\\0 & 0 & \cdots & \psi_p \end{pmatrix}$
    + $\mathrm{Cov}(\mathbf{F}, \mathbf{E}) = 0$
- In other words:
  + Both common and specific factors have mean zero;
  + They are uncorrelated;
  + The common factors are mutually uncorrelated and standardised;
  + The specific factors each affect only one observed variable.

## Structured Covariance {.allowframebreaks}

  - As a consequence of these assumptions, we can derive an assumption on the structure of $\Sigma = \mathrm{Cov}(\mathbf{Y})$:
  \begin{align*}
  \Sigma &= E\left( (\mathbf{Y} - \mu)(\mathbf{Y} - \mu)^T \right)\\
  &= E\left( (L \mathbf{F} + \mathbf{E})(L \mathbf{F} + \mathbf{E})^T \right)\\
  &= LE(\mathbf{F}\mathbf{F}^T)L + E(\mathbf{E}\mathbf{F}^T)L^T + LE(\mathbf{F}\mathbf{E}^T) + E(\mathbf{E}\mathbf{E}^T)\\
  &= LIL^T + 0L^T + L0 + \Psi\\
  &= LL^T + \Psi.
  \end{align*}
  - Similarly, we can show that
  $$ \mathrm{Cov}(\mathbf{Y}, \mathbf{F}) = L.$$
  - If we write $\ell_{ij}$ for the $(i,j)$-th element of $L$, we see that 
  $$ \mathrm{Var}(Y_i) = \sum_{k=1}^m \ell_{ik}^2 + \psi_i.$$
  - Crucially, these assumptions are **testable**. In other words, we can check whether they are reasonable for our data.
  
## Example {.allowframebreaks}

  - Let's look at an example where there is no solution. 
  - Assume $p=3$, $m=1$, with 
  $$\Sigma = \begin{pmatrix} 1 & 0.9 & 0.7\\0.9 & 1 & 0.4\\0.7 & 0.4 & 1\end{pmatrix}.$$
  - From our assumptions on the covariance structure, we derive a system of equations
  $$\begin{matrix}
  1 = \ell_{11}^2 + \psi_1 & 0.9 = \ell_{11}\ell_{21} & 0.7 = \ell_{11}\ell_{31} \\
  & 1 = \ell_{22}^2 + \psi_2 & 0.4= \ell_{21}\ell_{31}\\
  & & 1 = \ell_{33}^2 + \psi_3
  \end{matrix}$$
  - From $0.7 = \ell_{11}\ell_{31}$ and $0.4= \ell_{21}\ell_{31}$, we get 
  $$ \ell_{21} = \frac{0.4}{0.7}\ell_{11}.$$
  - But since $0.9 = \ell_{11}\ell_{21}$, we can conclude
  that
  $$ \ell_{11} = \pm 1.255.$$
  - However, since the first component $Y_1$ has unit variance, $\ell_{11} = \mathrm{Corr}(Y_1, F_1)$, and therefore the correlation is out of bounds.
  - Similarly, we get
  $$ \psi_1 = 1 - \ell_{11}^2 = 1 - 1.575 = -0.575.$$
  - But since $\psi_1$ is the variance of the first error term, we once again get a non-sensical solution.

## Factor Rotation {.allowframebreaks}

  - Even with our assumptions above, our model is still not uniquely identified.
  - Let $T$ be an $m\times m$ orthogonal matrix. We have
  \begin{align*}
  \mathbf{Y} - \mu &= L \mathbf{F} + \mathbf{E}\\
  &= L TT^T\mathbf{F} + \mathbf{E}\\
  &= \tilde{L}\tilde{\mathbf{F}} + \mathbf{E},\\
  \end{align*}
  where $\tilde{L}=LT$ and $\tilde{\mathbf{F}}=T^T\mathbf{F}$.
  - Both models lead to the same covariance matrix:
  $$ \Sigma = LL^T + \Psi = LTT^TL^T + \Psi = \tilde{L}\tilde{L}^T + \Psi.$$
  - As we will see, this will turn out to be a blessing in disguise:
    + We will impose a uniqueness condition to get one solution.
    + Then we will rotate our solution using $T$ to improve interpretation.
  
## Estimation--Principal Component Method {.allowframebreaks}

  - Recall the spectral decomposition of the covariance matrix:
  $$ \Sigma = \sum_{i=1}^p \lambda_i w_i w_i^T,$$
  with $\lambda_1\geq\cdots\geq\lambda_p$.
  - If we let $W$ be the matrix whose $i$-th column is $\sqrt{\lambda_i} w_i$, we can rewrite the spectral decomposition as
  $$ \Sigma = W W^T.$$
  - In other words, if we let $m=p$ and $\Psi = 0$, we see that we recover the orthogonal factor model with $L=W$.
  - Of course, this is not very satisfactory, as the dimension of the common factors is the same as that of the original data.
  - Instead, we select $m < p$ using one of the methods we discussed with PCA and we approximate
  $$\Sigma \approx \sum_{i=1}^m \lambda_i w_i w_i^T.$$
  - If we let $L$ be the $p\times m$ matrix whose $i$-th column is $\sqrt{\lambda_i} w_i$, we can estimate $\Psi$ as follows:
  $$\psi_i = \sigma_{ii} - \sum_{j=1}^m\ell_{ij}^2.$$

### Algorithm

  1. Let $\hat{\lambda}_1\>\cdots>\hat{\lambda}_p$ and $\hat{w}_1, \hat{w}_p$ be the eigenvalues and eigenvectors of the covariance matrix $S_n$.
  2. Select $m$ using one of the PCA criteria.
  3. Estimate $\hat{L}$ with the $p\times m$ matrix whose $i$-th column is $\sqrt{\hat{\lambda}_i} \hat{w}_i$.
  4. Estimate $\hat{\Psi}$ with the diagonal elements of $S_n - \hat{L}\hat{L}^T$.
  
## Example {.allowframebreaks}
  
```{r}
library(psych)

dim(bfi)
names(bfi)
```

```{r message = FALSE}
library(tidyverse)

data <- bfi %>% 
  select(-gender, -education, -age) %>% 
  filter(complete.cases(.))
```

```{r}
cor.plot(cor(data))
```

```{r}
decomp <- prcomp(data)
summary(decomp)$importance[,1:3]

cum_prop <- decomp %>% 
  summary %>% 
  .[["importance"]] %>% 
  .["Cumulative Proportion",]

which(cum_prop > 0.8)
```

```{r}
Lhat <- decomp$rotation[,1:14] %*% 
  diag(decomp$sdev[1:14])
Psi_hat <- diag(cov(data) - tcrossprod(Lhat))

sum((cov(data) - tcrossprod(Lhat) - diag(Psi_hat))^2)
sum(diag(cov(data)))
```

