---
title: "Principal Component Analysis"
draft: false
source: true
output: binb::metropolis
fontsize: 12pt
author: Max Turgeon
institute: STAT 4690--Applied Multivariate Analysis
header-includes:
  - \usefonttheme{professionalfonts}
  - \usepackage{graphicx}
  - \usepackage{tikzpagenodes}
  - \usetikzlibrary{calc}
  - \usepackage{caption}
  - \usepackage{xcolor}
---

```{r,setup, include=FALSE}
knitr::opts_chunk$set(cache=FALSE)
```

## Population PCA {.allowframebreaks}

  - **PCA**: Principal Component Analysis
  - Dimension reduction method: 
    + Let $\mathbf{Y} = (Y_1, \ldots, Y_p)$ be a random vector with covariance matrix $\Sigma$. We are looking for a transformation $h: \mathbb{R}^p \to \mathbb{R}^k$, with $k\ll p$ such that $h(\mathbf{Y})$ retains "as much information as possible" about $\mathbf{Y}$.
  - In PCA, we are looking for a **linear transformation** $h(y) = w^Ty$ with **maximal variance** (where $\|w\|=1$)
  - More generally, we are looking for $k$ linear transformations $w_1, \ldots, w_k$ such that $w_j^T\mathbf{Y}$ has maximal variance and is uncorrelated with $w_1^T\mathbf{Y},\ldots,w_{j-1}^T\mathbf{Y}$.
  - First, note that $\mathrm{Var}(w^T\mathbf{Y}) = w^T\Sigma w$. So our optimisation problem is
  $$ \max_w w^T\Sigma w, \quad\mbox{with } w^Tw = 1.$$
  - From the theory of Lagrange multipliers, we can look at the *unconstrained* problem 
  $$ \max_{w,\lambda} w^T\Sigma w - \lambda (w^Tw - 1).$$
  - Write $\phi(w, \lambda)$ for the function we are trying to optimise. We have
  \begin{align*}
  \frac{\partial}{\partial w} \phi(w, \lambda) &= \frac{\partial}{\partial w}w^T\Sigma w - \lambda (w^Tw - 1)\\
  &= 2\Sigma w - 2\lambda w;\\
  \frac{\partial}{\partial \lambda} \phi(w, \lambda) &= w^Tw - 1.
  \end{align*}
  - From the first partial derivative, we conclude that
  $$ \Sigma w = \lambda w.$$
  - From the second partial derivative, we conclude that $w\neq 0$; in other words, $w$ is an eigenvector of $\Sigma$ with eigenvalue $\lambda$.
  - Moreover, at this stationary point of $\phi(w, \lambda)$, we have
  $$\mathrm{Var}(w^T\mathbf{Y})=w^T\Sigma w = w^T(\lambda w) = \lambda w^Tw = \lambda.$$
  - In other words, to maximise the variance $\mathrm{Var}(w^T\mathbf{Y})$, we need to choose $\lambda$ to be the *largest* eigenvalue of $\Sigma$.
  - By induction, and using the extra constraints $w_i^Tw_j = 0$, we can show that all other linear transformations are given by eigenvectors of $\Sigma$.
  
### PCA Theorem

Let $\lambda_1\geq\cdots\geq\lambda_p$ be the eigenvalues of $\Sigma$, with corresponding unit-norm eigenvectors $w_1,\ldots, w_p$. To reduce the dimension of $\mathbf{Y}$ from $p$ to $k$ such that every component of $W^T\mathbf{Y}$ is uncorrelated and each direction has maximal variance, we can take $W=\begin{pmatrix} w_1&\cdots&w_k\end{pmatrix}$, whose $j$-th column is $w_j$.

## Properties of PCA {.allowframebreaks}

  - Some vocabulary:
    + $\mathbf{Z}_i = w_i^T\mathbf{Y}$ is called the $i$-th **principal component** of $\mathbf{Y}$.
    + $w_i$ is the $i$-th vector of **loadings**.
  - Note that we can take $k=p$, in which case we do not reduce the dimension of $\mathbf{Y}$, but we *transform* it into a random vector with uncorrelated components.
  - Let $\Sigma = P\Lambda  P^T$ be the eigendecomposition of $\Sigma$. We have
  $$\sum_{i=1}^p \mathrm{Var}(w_i^T\mathbf{Y}) = \sum_{i=1}^p \lambda_i = \mathrm{tr}(\Lambda) = \mathrm{tr}(\Sigma) = \sum_{i=1}^p \mathrm{Var}(Y_i).$$
  - Therefore, each linear transformation $w_i^T\mathbf{Y}$ contributes $\lambda_i/\sum_j \lambda_j$ as percentage of the overall variance.
  - **Selecting $k$**: One common strategy is to select a threshold (e.g. $c = 0.9$) such that
  $$\frac{\sum_{i=1}^{{\color{blue}k}} \lambda_i}{\sum_{i=1}^p \lambda_i} \geq c.$$
  
## Scree plot

  - A **scree plot** is a plot with the sequence $1,\ldots,p$ on the x-axis, and the sequence $\lambda_1, \ldots, \lambda_p$ on the y-axis.
  - Another common strategy for selecting $k$ is to choose the point where the curve starts to flatten out.
    + **Note**: This inflection point does not necessarily exist, and it may be hard to identify.

---

```{r echo = FALSE}
C <- chol(S <- toeplitz(.9 ^ (0:31))) # Cov.matrix and its root
set.seed(17)
X <- matrix(rnorm(32000), 1000, 32)
Z <- X %*% C  ## ==>  cov(Z) ~=  C'C = S
pZ <- prcomp(Z, tol = 0.1)
screeplot(pZ, type = 'lines', main = '')
```

## Correlation matrix

  - When the observations are on the different scale, it is typically more appropriate to normalise the components of $\mathbf{Y}$ before doing PCA.
    + The variance depends on the units, and therefore without normalising, the component with the "smallest" units (e.g. centimeters vs. meters) could be driving most of the overall variance.
  - In other words, instead of using $\Sigma$, we can use the (population) correlation matrix $R$.
  - **Note**: The loadings and components we obtain from $\Sigma$ are **not** equivalent to the ones obtained from $R$.

## Sample PCA

  - In general, we do not the population covariance matrix $\Sigma$.
  - Therefore, in practice, we estimate the loadings $w_i$ through the eigenvectors of the sample covariance matrix $S_n$. 
  - As with the population version of PCA, if the units are different, we should normalise the components or use the sample correlation matrix.

## Example 1 {.allowframebreaks}

```{r}
library(mvtnorm)
Sigma <- matrix(c(1, 0.5, 0.1,
                  0.5, 1, 0.5,
                  0.1, 0.5, 1),
                ncol = 3) 

set.seed(17)
X <- rmvnorm(100, sigma = Sigma)
pca <- prcomp(X)
```

```{r}
summary(pca)
```

```{r}
screeplot(pca, type = 'l')
```

## Example 2 {.allowframebreaks}

```{r}
pca <- prcomp(USArrests, scale = TRUE)
```

```{r}
summary(pca)
```

```{r}
screeplot(pca, type = 'l')
```


# Applications of PCA

## Training and testing {.allowframebreaks}

  - Recall: **Mean Squared Error**
  $$ MSE = \frac{1}{n}\sum_{i=1}^n(Y_i - \hat{Y}_i)^2,$$
  where $Y_i,\hat{Y}_i$ are the *observed* and *predicted* values.
  - It is good practice to separate your dataset in two:
    + **Training** dataset, that is used to build and fit your model (e.g. choose covariates, estimate regression coefficients).
    + **Testing** dataset, that it used to compute the MSE or other performance metrics.
  - PCA can be used for predictive model building in (univariate) linear regression:
    + **Feature extraction**: Perform PCA on the covariates, extract the first $k$ PCs, and use them as predictors in your model.
    + **Feature selection**: Perform PCA on the covariates, look at the first PC, find the covariates whose loadings are the largest (in absolute value), and only use those covariates as predictors.

## Feature Extraction {.allowframebreaks}

```{r message = FALSE}
library(ElemStatLearn)
library(tidyverse)
train <- subset(prostate, train == TRUE,
                select = -train)
test  <- subset(prostate, train == FALSE,
                select = -train)

# First model: Linear regression
lr_model <- lm(lpsa ~ ., data = train)
lr_pred <- predict(lr_model, newdata = test)
(lr_mse <- mean((test$lpsa - lr_pred)^2))

# PCA
decomp <- train %>%
  subset(select = -lpsa) %>%
  as.matrix() %>%
  prcomp
summary(decomp)$importance[,1:3]

screeplot(decomp, type = 'lines')
```


```{r}
# Second model: PCs for predictors
train_pc <- train
train_pc$PC1 <- decomp$x[,1]
pc_model <- lm(lpsa ~ PC1, data = train_pc)
```


```{r}
test_pc <- as.data.frame(predict(decomp, test))
pc_pred <- predict(pc_model,
                   newdata = test_pc)
(pc_mse <- mean((test$lpsa - pc_pred)^2))
```

## Feature Selection {.allowframebreaks}

```{r}
contribution <- decomp$rotation[,"PC1"]
round(contribution, 3)[1:6]
round(contribution, 3)[7:8]

(keep <- names(which(abs(contribution) > 0.01)))

fs_model <- lm(lpsa ~ ., data = train[,c(keep, "lpsa")])
fs_pred <- predict(fs_model, newdata = test)
(fs_mse <- mean((test$lpsa - fs_pred)^2))
```

```{r, message = FALSE}
model_plot <- data.frame(
  "obs" = test$lpsa,
  "LR" = lr_pred,
  "PC" = pc_pred,
  "FS" = fs_pred
) %>%
  gather(Model, pred, -obs)
```


```{r, message = FALSE}
ggplot(model_plot,
       aes(pred, obs, colour = Model)) +
  geom_point() +
  theme_minimal() +
  geom_abline(slope = 1, intercept = 0) +
  theme(legend.position = 'top') +
  xlab("Predicted") + ylab("Observed")
```

## Comments

  - The full model performed better than the ones we created with PCA
    + It had a lower MSE
  - On the other hand, if we had multicollinearity issues, or too many covariates ($p > n$), the PCA models could outperform the full model.
  - However, note that PCA does not use the association between the covariates and the outcome, so it will never be the most efficient way of building a model.

## Data Visualization {.allowframebreaks}

```{r mnist, cache = TRUE}
library(dslabs)

mnist <- read_mnist()

dim(mnist$train$images)
dim(mnist$test$images)

head(mnist$train$labels)
```


```{r}
matrix(mnist$train$images[1,], ncol = 28) %>%
  image(col = gray.colors(12, rev = TRUE),
        axes = FALSE)
```

```{r cache = TRUE}
decomp <- prcomp(mnist$train$images)
```

```{r}
screeplot(decomp, type = 'lines',
          npcs = 20, main = "")
```

```{r}
decomp$x[,1:2] %>%
  as.data.frame() %>%
  mutate(label = factor(mnist$train$labels)) %>%
  ggplot(aes(PC1, PC2, colour = label)) +
  geom_point(alpha = 0.5) +
  theme_minimal()
```

```{r}
# And on the test set
decomp %>%
  predict(newdata = mnist$test$images) %>%
  as.data.frame() %>%
  mutate(label = factor(mnist$test$labels)) %>%
  ggplot(aes(PC1, PC2, colour = label)) +
  geom_point(alpha = 0.5) +
  theme_minimal()
```

```{r}
par(mfrow = c(2, 2))
for (i in seq_len(4)) {
  matrix(decomp$rotation[,i], ncol = 28) %>%
  image(col = gray.colors(12, rev = TRUE),
        axes = FALSE, main = paste0("PC", i))
}
```

```{r}
# Approximation with 90 PCs
approx_mnist <- decomp$rotation[, seq_len(90)] %*% 
  decomp$x[1, seq_len(90)]
par(mfrow = c(1, 2))

matrix(mnist$train$images[1,], ncol = 28) %>%
  image(col = gray.colors(12, rev = TRUE),
        axes = FALSE, main = "Original")
matrix(approx_mnist, ncol = 28) %>%
  image(col = gray.colors(12, rev = TRUE),
        axes = FALSE, main = "Approx")
```


## Additional comments about sample PCA {.allowframebreaks}

  - Let $\mathbf{Y}_1, \ldots, \mathbf{Y}_n$ be a sample from a distribution with covariance matrix $\Sigma$. Write $\mathbb{Y}$ for the $n\times p$ matrix whose $i$-th row is $\mathbf{Y}_i$.
  - Let $S_n$ be the sample covariance matrix, and write $W_k$ for the matrix whose columns are the first $k$ eigenvectors of $S_n$. 
  - You can define the matrix of $k$ principal components as
  $$ \mathbb{Z} = \mathbb{Y} W_k.$$
  - On the other hand, it is much more common to define it as
  $$\mathbb{Z} = \tilde{\mathbb{Y}} W_k,$$
  where $\tilde{\mathbb{Y}}$ is the centered version of $\mathbb{Y}$ (i.e. the sample mean has been subtracted from each row).
    + This leads to sample principal components with mean zero.
  
## Example 1 (revisited) {.allowframebreaks}

```{r}
library(mvtnorm)
Sigma <- matrix(c(1, 0.5, 0.1,
                  0.5, 1, 0.5,
                  0.1, 0.5, 1),
                ncol = 3) 
mu <- c(1, 2, 2)
```


```{r}
set.seed(17)
X <- rmvnorm(100, mean = mu,
             sigma = Sigma)
pca <- prcomp(X)

colMeans(X)
colMeans(pca$x)
```

```{r}
# On the other hand
pca <- prcomp(X, center = FALSE)
colMeans(pca$x)
```

## Geometric interpretation of PCA {.allowframebreaks}

  - The definition of PCA as a linear combination that maximises variance is due to Hotelling (1933).
  - But PCA was actually introduced earlier by Pearson (1901)
    + *On Lines and Planes of Closest Fit to Systems of Points in Space*
  - He defined PCA as the **best approximation of the data by a linear manifold**
  - Let's suppose we have a lower dimension representation of $\mathbb{Y}$, denoted by a $n\times k$ matrix $\mathbb{Z}$.
  - We want to *reconstruct* $\mathbb{Y}$ using an affine transformation 
  $$ f(z) = \mu + W_k z,$$
  where $W_k$ is a $p\times k$ matrix.
  - We want to find $\mu, W_k, \mathbf{Z}_i$ that minimises the **reconstruction error**:
  $$ \min_{\mu, W_k, \mathbf{Z}_i} \sum_{i=1}^n \| \mathbf{Y}_i - \mu - W_k \mathbf{Z}_i \|^2.$$
  - First, treating $W_k$ constant and minimising over $\mu, \mathbf{Z}_i$, we find
  \begin{align*}
  \hat{\mu} &= \mathbf{\bar{Y}},\\
  \hat{\mathbf{Z}}_i &= W_k^T(\mathbf{Y}_i - \mathbf{\bar{Y}}).
  \end{align*}
  - Putting these quantities into the reconstruction error, we get
  $$ \min_{W_k} \sum_{i=1}^n \| (\mathbf{Y}_i - \mathbf{\bar{Y}}) - W_k W_k^T(\mathbf{Y}_i - \mathbf{\bar{Y}}) \|^2.$$

### Eckart–Young theorem

The reconstruction error is minimised by taking $W_k$ to be the matrix whose columns are the first $k$ eigenvectors of the sampling covariance matrix $S_n$.

Equivalently, we can take the matrix whose columns are the first $k$ *right singular vectors* or the centered data matrix $\tilde{\mathbb{Y}}.$

## Example {.allowframebreaks}

```{r}
set.seed(1234)
# Random measurement error
sigma <- 5

# Exact relationship between 
# Celsius and Fahrenheit
temp_c <- seq(-40, 40, by = 1)
temp_f <- 1.8*temp_c + 32
```


```{r}
# Add measurement error
temp_c_noise <- temp_c + rnorm(n = length(temp_c), 
                               sd = sigma)
temp_f_noise <- temp_f + rnorm(n = length(temp_f), 
                               sd = sigma)
```


```{r}
# Linear model
(fit <- lm(temp_f_noise ~ temp_c_noise))
confint(fit)

# PCA
pca <- prcomp(cbind(temp_c_noise, temp_f_noise))
pca$rotation
pca$rotation[2,"PC1"]/pca$rotation[1,"PC1"]
```

